\section{Research questions}
\paragraph{Which strategy for game playing do you use?} The strategy used for playing is Monte Carlo tree search, extended with the different optimisations discussed in Section \ref{s:mcts}.
	
\paragraph{Which data representation do you use?} Board representation is discussed in Section \ref{s:board}.

\paragraph{Which machine learning model(s) do you use to represent the game state?} We use an Artificial Neural Network, as discussed in Section \ref{s:ann}.
	
\paragraph{How do you force a decision within a given time limit?} The search in the Monte Carlo search tree for a move is a iterative process, that can be stopped when the time limit is reached. Because we want to take some small overhead time into account, we reduced the time limit with 0.02 seconds to make sure the decision is made in time.
	
\paragraph{How will you evaluate your solution?} Evaluation is described in Section \ref{s:evaluation}.
	
\paragraph{What is the computational and memory cost of preprocessing, learning and evaluating?} % TODO
	
\paragraph{Can you represent and learn to recognize the concept of chains, a popular strategy in ``Dots and Boxes''?} Yes, as described in Section \ref{s:chains}.
	
\paragraph{How does your best game playing strategy compare to your other strategies (performance)?} A comparison of our different playing strategies can be found in Section \ref{s:evaluationOwn}.
	
\paragraph{What is the performance/time/space trade-off between your different strategies?} Agent 1 until 3 use a basic board representation that only records the edges that have been played and the score of the players. The board representation used in Agent 4-6 can represent chains and stores more data. The first board representation has a lower time complexity when registering a move, but has a higher time complexity when it is asked for all the moves that can be played from that board. The reverse is true for the second board representation.
