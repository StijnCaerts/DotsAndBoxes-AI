\section{Overall evaluation}
% Put games won/lost/tied as first/second player against other AIs here

\subsection{Evaluation of own implementations}
The different optimisations described in the previous section were implemented in an incremental fashion (except for the asynchronous search, which wasn't applied to the agent with the artificial neural network). This gives us the following list of agents that we can test:
\begin{description}
	\item[Agent 1] Monte Carlo tree search
	\item[Agent 2] MCTS with early simulation termination
	\item[Agent 3] Agent 2 extended with reuse of the search tree
	\item[Agent 4] Agent 3 extended with the use of optimal moves
	\item[Agent 5] Agent 4 extended with asynchronous search
	\item[Agent 6] Agent 4 extended with ANN heuristic
\end{description}

To measure the difference in performance between the optimisations we made, we made the different agents compete with each other. All tested combinations of two different agents played 100 games on a board of a given size against each other. The player who starts first is alternated every game, to make it a fair comparison between the two agents.
We repeated this experiment for board sizes of 5x5 and 6x6.

\subsubsection{Agent 1 vs Agent 2}
First, the basic Monte Carlo tree search implementation competed with the optimisation that has early termination of simulation. 100 games were played on a board size of 5x5, and another 100 games were played with a board size of 6x6. The results of these games are shown in Table \ref{result:Ag1vsAg2}.

\begin{table}[!h]
	\centering
	\begin{tabular}{c | c | c | c}
		\textit{5x5} & \multicolumn{3}{c}{\textbf{Winner}}        \\
		\textbf{Playing first} & Agent 1 & Tie & Agent 2 \\ \hline
		Agent 1 & 18 & 0 & 32 \\ \hline
		Agent 2 & 24 & 0 & 26
	\end{tabular}
	\quad
	\begin{tabular}{c | c | c | c}
		\textit{6x6} & \multicolumn{3}{c}{\textbf{Winner}}        \\
		\textbf{Playing first} & Agent 1 & Tie & Agent 2 \\ \hline
		Agent 1 & 19 & 0 & 31 \\ \hline
		Agent 2 & 20 & 0 & 30
	\end{tabular}
	\caption{\label{result:Ag1vsAg2}Agent 1 vs Agent 2}
\end{table}

As we can see in the results, Agent 2 is slightly better than Agent 1. This was more or less expected, as shorter simulations means that more simulations can be completed within the same time frame. Implementing the early termination of simulation was a good move, as more simulations leads eventually to better results.

\subsubsection{Agent 2 vs Agent 3}
We compare the agent with early termination of simulation and the agent with the extra feature of search tree reuse. The results of these simulations can be found in Table \ref{result:Ag2vsAg3}.

\begin{table}[!h]
	\centering
	\begin{tabular}{c | c | c | c}
		\textit{5x5} & \multicolumn{3}{c}{\textbf{Winner}}        \\
		\textbf{Playing first} & Agent 2 & Tie & Agent 3 \\ \hline
		Agent 2 & 25 & 0 & 25 \\ \hline
		Agent 3 & 28 & 0 & 22
	\end{tabular}
	\quad
	\begin{tabular}{c | c | c | c}
		\textit{6x6} & \multicolumn{3}{c}{\textbf{Winner}}        \\
		\textbf{Playing first} & Agent 2 & Tie & Agent 3 \\ \hline
		Agent 2 & 24 & 0 & 26 \\ \hline
		Agent 3 & 20 & 0 & 30
	\end{tabular}
	\caption{\label{result:Ag2vsAg3}Agent 2 vs Agent 3}
\end{table}

The impact of the reuse of the search tree is rather small, and not in the direction that we expected. The slightly worse results are possibly caused by the overhead of looking for the node that corresponds with the move that has been played, among the children of the root node in the search tree. The high branching factor isn't helping here either.


\subsubsection{Agent 3 vs Agent 4}
Agent 4 can make use of the optimal moves it computes to handle chains better and this is immediately clear from the test results, as shown in Table \ref{result:Ag3vsAg4}. Agent 4 manages to win all games played against Agent 3. This shows that having knowledge of chains in the Monte Carlo tree search is a very useful asset.

\begin{table}[!h]
	\centering
	\begin{tabular}{c | c | c | c}
		\textit{5x5} & \multicolumn{3}{c}{\textbf{Winner}}        \\
		\textbf{Playing first} & Agent 3 & Tie & Agent 4 \\ \hline
		Agent 3 & 0 & 0 & 50 \\ \hline
		Agent 4 & 0 & 0 & 50
	\end{tabular}
	\quad
	\begin{tabular}{c | c | c | c}
		\textit{6x6} & \multicolumn{3}{c}{\textbf{Winner}}        \\
		\textbf{Playing first} & Agent 3 & Tie & Agent 4 \\ \hline
		Agent 3 & 0 & 0 & 50 \\ \hline
		Agent 4 & 0 & 0 & 50
	\end{tabular}
	\caption{\label{result:Ag3vsAg4}Agent 3 vs Agent 4}
\end{table}


\subsubsection{Agent 4 vs Agent 5}
Agent 5 is an extension of Agent 4 in the sense that it also searches the tree for good moves while the opponent is thinking. But because the branching factor of a typical game of ``Dots and Boxes'' is very large, searching before the opponent has made a move only makes sense when we can reduce the set of moves of which the other player is likely to select a move. This can be done by only considering the optimal moves that the opponent can make as a basis for this asynchronous search.

In the results (Table \ref{result:Ag4vsAg5}), we see that the difference between Agent 4 and Agent 5 is not big. However, Agent 5 tends to perform worse in the tests we have conducted.

\begin{table}[!h]
	\centering
	\begin{tabular}{c | c | c | c}
		\textit{5x5} & \multicolumn{3}{c}{\textbf{Winner}}        \\
		\textbf{Playing first} & Agent 4 & Tie & Agent 5 \\ \hline
		Agent 4 & 30 & 0 & 20 \\ \hline
		Agent 5 & 25 & 0 & 25
	\end{tabular}
	\quad
	\begin{tabular}{c | c | c | c}
		\textit{6x6} & \multicolumn{3}{c}{\textbf{Winner}}        \\
		\textbf{Playing first} & Agent 4 & Tie & Agent 5 \\ \hline
		Agent 4 & 28 & 0 & 22 \\ \hline
		Agent 5 & 28 & 0 & 22
	\end{tabular}
	\caption{\label{result:Ag4vsAg5}Agent 4 vs Agent 5}
\end{table}


\subsubsection{Agent 4 vs Agent 6}
Agent 6 uses an artificial neural network as a heuristic in the Monte Carlo tree search. Agent 6 doesn't use asynchronous search, and thus is an extension of Agent 4. That's the reason we decided it would make more sense to evaluate this agent against the agent it is based on. In Table \ref{result:Ag4vsAg6}, the comparison of both agents is shown.

\begin{table}[!h]
	\centering
	\begin{tabular}{c | c | c | c}
		\textit{5x5} & \multicolumn{3}{c}{\textbf{Winner}}        \\
		\textbf{Playing first} & Agent 4 & Tie & Agent 6 \\ \hline
		Agent 4 & 25 & 0 & 25 \\ \hline
		Agent 6 & 25 & 0 & 25
	\end{tabular}
	\quad
	\begin{tabular}{c | c | c | c}
		\textit{6x6} & \multicolumn{3}{c}{\textbf{Winner}}        \\
		\textbf{Playing first} & Agent 4 & Tie & Agent 6 \\ \hline
		Agent 4 & 26 & 0 & 24 \\ \hline
		Agent 6 & 22 & 1 & 27
	\end{tabular}
	\caption{\label{result:Ag4vsAg6}Agent 4 vs Agent 6}
\end{table}

From the test results, we can conclude that Agent 6 performs just as well as Agent 4. There are some slight deviations, in both sides, but these have no significant meaning.


\subsection{Evaluation against other AI's}
To evaluate the performance of our agents, we let them compete to other AI's for the game of ``Dots and Boxes'' that are available online. We chose the following three websites to conduct our evaluation:
\begin{itemize}
	\item \url{http://dotsandboxes.org/}
	\item \url{http://dotsgame.co/}
	\item \url{http://www.math.ucla.edu/~tom/Games/dots&boxes.html}
\end{itemize}

We only evaluated our two best performing agents against the online AI's, namely Agent 4 and Agent 6.
Because all moves had to be transferred by hand from the board from our agent to the board of the online agent, we limited the number of games per agent against a particular online AI to 6. This is not enough to give a conclusive result about the performance of our agents relative to the online AI's, but it was enough to show us some trends.

\begin{table}[!h]
	\centering
	\begin{tabular}{c | c | c | c}
		\textit{5x5} & \multicolumn{3}{c}{\textbf{Winner}}        \\
		\textbf{Playing first} & Agent 4 & Tie & Other AI \\ \hline
		Agent 4 & 0 & 0 & 3 \\ \hline
		Other AI & 0 & 0 & 3
	\end{tabular}
	\quad
	\begin{tabular}{c | c | c | c}
		\textit{5x5} & \multicolumn{3}{c}{\textbf{Winner}}        \\
		\textbf{Playing first} & Agent 6 & Tie & Other AI \\ \hline
		Agent 6 & 0 & 0 & 3 \\ \hline
		Other AI & 0 & 0 & 3
	\end{tabular}
	\caption{\label{result:dotsandboxesorg}Playing against \url{http://dotsandboxes.org/}}
\end{table}


\begin{table}[!h]
	\centering
	\begin{tabular}{c | c | c | c}
		\textit{4x5} & \multicolumn{3}{c}{\textbf{Winner}}        \\
		\textbf{Playing first} & Agent 4 & Tie & Other AI \\ \hline
		Agent 4 & 2 & 0 & 4
	\end{tabular}
	\quad
	\begin{tabular}{c | c | c | c}
		\textit{4x5} & \multicolumn{3}{c}{\textbf{Winner}}        \\
		\textbf{Playing first} & Agent 6 & Tie & Other AI \\ \hline
		Agent 6 & 2 & 0 & 4
	\end{tabular}
	\caption{\label{result:dotsgameco}Playing against \url{http://dotsgame.co/}}
\end{table}


\begin{table}[!h]
	\centering
	\begin{tabular}{c | c | c | c}
		\textit{5x5} & \multicolumn{3}{c}{\textbf{Winner}}        \\
		\textbf{Playing first} & Agent 4 & Tie & Other AI \\ \hline
		Agent 4 & 1 & 0 & 5
	\end{tabular}
	\quad
	\begin{tabular}{c | c | c | c}
		\textit{5x5} & \multicolumn{3}{c}{\textbf{Winner}}        \\
		\textbf{Playing first} & Agent 6 & Tie & Other AI \\ \hline
		Agent 6 & 2 & 0 & 4
	\end{tabular}
	\caption{\label{result:uclaTom}Playing against \url{http://www.math.ucla.edu/~tom/Games/dots&boxes.html}}
\end{table}

In the results, shown in Tables \ref{result:dotsandboxesorg}, \ref{result:dotsgameco} and \ref{result:uclaTom}, it is clear that our agent lost the games most of the time. However, we still gained some valuable insights from this evaluation. First, we see that our agents are capable to recognize chains and play accordingly to these chains. But often, our agents fail to get a controlling position in the end game when the chains are played. Occasionally when we do have a controlling position, our agent makes and odd decision and loses control by playing a bad move.